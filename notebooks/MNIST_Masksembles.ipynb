{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dacca9e1"
      },
      "source": [
        "\n",
        "# Masksembles on MNIST â€” Tutorial\n",
        "\n",
        "Welcome! ðŸ‘‹\n",
        "\n",
        "This notebook walks you through using **Masksembles** for uncertaintyâ€‘aware inference on the classic **MNIST** dataset.  \n",
        "\n",
        "You'll see how to:\n",
        "\n",
        "- Load and preprocess MNIST\n",
        "- Define **Masksembles** layers (`Masksembles2D` / `Masksembles1D`)\n",
        "- Build a small CNN\n",
        "- Train and validate in PyTorch\n",
        "- Run **perâ€‘submodel predictions** (ensembled behavior) at inference time\n",
        "- Avoid common pitfalls\n",
        "\n",
        "> **What is Masksembles?**  \n",
        "> Masksembles builds multiple *subnetworks* inside a single model using deterministic, nonâ€‘overlapping masks. At inference time you can obtain diverse predictions (like an ensemble) **without** keeping multiple separate models. This gives you better **uncertainty estimation** and often more robust predictions with minimal overhead.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "670b136c"
      },
      "source": [
        "\n",
        "## Table of Contents\n",
        "1. [Setup & Imports](#setup-imports)\n",
        "2. [Load MNIST & Preprocessing](#load-preprocess)\n",
        "3. [Masksembles Layers](#masksembles-layers)\n",
        "4. [Model Architecture](#model-architecture)\n",
        "5. [Training Setup](#training-setup)\n",
        "6. [Training Loop](#training-loop)\n",
        "7. [Evaluation & Inference](#eval-infer)\n",
        "8. [Next Steps](#next-steps)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But before we start, lets make sure you installed Masksembles package."
      ],
      "metadata": {
        "id": "YuRK1pIbrBL0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qrpn5UV2BYLF",
        "outputId": "b1634cd5-3287-45bc-de91-88c0ba806fd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting masksembles\n",
            "  Downloading masksembles-1.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading masksembles-1.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: masksembles\n",
            "Successfully installed masksembles-1.1\n",
            "--2025-11-09 12:52:18--  https://github.com/nikitadurasov/masksembles/raw/main/images/complex_sample_mnist.npy\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/nikitadurasov/masksembles/main/images/complex_sample_mnist.npy [following]\n",
            "--2025-11-09 12:52:18--  https://raw.githubusercontent.com/nikitadurasov/masksembles/main/images/complex_sample_mnist.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6400 (6.2K) [application/octet-stream]\n",
            "Saving to: â€˜complex_sample_mnist.npyâ€™\n",
            "\n",
            "complex_sample_mnis 100%[===================>]   6.25K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-11-09 12:52:18 (96.2 MB/s) - â€˜complex_sample_mnist.npyâ€™ saved [6400/6400]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install masksembles\n",
        "!wget https://github.com/nikitadurasov/masksembles/raw/main/images/complex_sample_mnist.npy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32ca1561"
      },
      "source": [
        "\n",
        "## 1. Setup & Imports <a id=\"setup-imports\"></a>\n",
        "\n",
        "This section imports PyTorch, TorchVision, and the other packages used later.  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "r4tPNSFvqUWp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be8e329c"
      },
      "source": [
        "\n",
        "## 2. Load MNIST & Preprocessing <a id=\"load-preprocess\"></a>\n",
        "\n",
        "Here we load MNIST and transform images to **tensors in [0, 1]**.  \n",
        "Remember that **PyTorch uses channelâ€‘first** tensors *(N, C, H, W)*, so MNIST images should become shape `(N, 1, 28, 28)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (1, 28, 28)  # PyTorch uses channel-first format (C, H, W)\n",
        "\n",
        "# Define transform: convert to tensor and normalize to [0, 1]\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Converts to tensor and scales to [0, 1]\n",
        "])\n",
        "\n",
        "# Load MNIST dataset\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    transform=transform,\n",
        "    download=True\n",
        ")\n",
        "test_dataset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    transform=transform,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Check shapes\n",
        "images, labels = next(iter(train_loader))\n",
        "print(\"x_train shape:\", images.shape)  # [batch_size, 1, 28, 28]\n",
        "print(len(train_dataset), \"train samples\")\n",
        "print(len(test_dataset), \"test samples\")\n",
        "\n",
        "# Convert labels to one-hot if needed\n",
        "# (usually not needed for CrossEntropyLoss)\n",
        "y_one_hot = torch.nn.functional.one_hot(labels, num_classes=num_classes).float()\n",
        "print(\"y_train one-hot shape:\", y_one_hot.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cs0l4BmmpuWm",
        "outputId": "04a6eef1-f807-4305-db7c-27677aed40f9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:00<00:00, 19.7MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 480kB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:00<00:00, 4.46MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 9.13MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: torch.Size([64, 1, 28, 28])\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "y_train one-hot shape: torch.Size([64, 10])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3. Masksembles Layers <a id=\"masksembles-layers\"></a>\n",
        "\n",
        "These classes (`Masksembles2D`, `Masksembles1D`) create **nonâ€‘overlapping channel masks** that define *submodels* within the same network.  \n",
        "- `Masksembles2D`: apply masks on convolutional feature maps (CÃ—HÃ—W).  \n",
        "- `Masksembles1D`: apply masks on flattened features (C).  \n",
        "Key arguments:\n",
        "- `channels`: number of channels in the input.\n",
        "- `n`: number of submodels (masks).\n",
        "- `scale`: controls correlation/capacity tradeâ€‘off between submodels.\n"
      ],
      "metadata": {
        "id": "1AgTlby2p0g-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from masksembles.torch import Masksembles2D, Masksembles1D"
      ],
      "metadata": {
        "id": "lAnwkpGtp3qP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr5xjOSffo7x"
      },
      "source": [
        "In order to transform regular model into Masksembles model one should add Masksembles2D or Masksembles1D layers in it. General recommendation is to insert these layers right before or after convolutional layers.\n",
        "\n",
        "In example below we'll use both Masksembles2D and Masksembles1D layers applied after convolutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75491b02"
      },
      "source": [
        "\n",
        "## 4. Model Architecture <a id=\"model-architecture\"></a>\n",
        "\n",
        "We define a small CNN with ELU activations and interleave **Masksembles** after conv and before the final linear layer.  \n",
        "After two conv+pool blocks, MNIST's 28Ã—28 maps down to 5Ã—5; the final linear projects to 10 classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from masksembles import common\n",
        "\n",
        "\n",
        "class Masksembles2D(nn.Module):\n",
        "    \"\"\"\n",
        "    :class:`Masksembles2D` is high-level class that implements Masksembles approach\n",
        "    for 2-dimensional inputs (similar to :class:`torch.nn.Dropout2d`).\n",
        "\n",
        "    :param channels: int, number of channels used in masks.\n",
        "    :param n: int, number of masks\n",
        "    :param scale: float, scale parameter similar to *S* in [1]. Larger values decrease \\\n",
        "        subnetworks correlations but at the same time decrease capacity of every individual model.\n",
        "\n",
        "    Shape:\n",
        "        * Input: (N, C, H, W)\n",
        "        * Output: (N, C, H, W) (same shape as input)\n",
        "\n",
        "    Examples:\n",
        "\n",
        "    >>> m = Masksembles2D(16, 4, 2.0)\n",
        "    >>> input = torch.ones([4, 16, 28, 28])\n",
        "    >>> output = m(input)\n",
        "\n",
        "    References:\n",
        "\n",
        "    [1] `Masksembles for Uncertainty Estimation`,\n",
        "    Nikita Durasov, Timur Bagautdinov, Pierre Baque, Pascal Fua\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, channels: int, n: int, scale: float):\n",
        "        super().__init__()\n",
        "        self.channels, self.n, self.scale = channels, n, scale\n",
        "        masks_np = common.generation_wrapper(channels, n, scale)  # numpy float64 by default\n",
        "        masks = torch.as_tensor(masks_np, dtype=torch.float32)    # make float32 here\n",
        "        self.register_buffer('masks', masks, persistent=False)    # not trainable, moves with .to()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # make sure masks match dtype/device (usually already true because of buffer, but safe)\n",
        "        masks = self.masks.to(dtype=inputs.dtype, device=inputs.device)\n",
        "\n",
        "        batch = inputs.shape[0]\n",
        "        # safer split even if batch % n != 0\n",
        "        chunks = torch.chunk(inputs.unsqueeze(1), self.n, dim=0)  # returns nearly equal chunks\n",
        "        x = torch.cat(chunks, dim=1).permute(1, 0, 2, 3, 4)       # [n, ?, C, H, W]\n",
        "        x = x * masks.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)    # broadcast masks\n",
        "        x = torch.cat(torch.split(x, 1, dim=0), dim=1)\n",
        "        return x.squeeze(0)\n",
        "\n",
        "\n",
        "\n",
        "class Masksembles1D(nn.Module):\n",
        "    \"\"\"\n",
        "    :class:`Masksembles1D` is high-level class that implements Masksembles approach\n",
        "    for 1-dimensional inputs (similar to :class:`torch.nn.Dropout`).\n",
        "\n",
        "    :param channels: int, number of channels used in masks.\n",
        "    :param n: int, number of masks\n",
        "    :param scale: float, scale parameter similar to *S* in [1]. Larger values decrease \\\n",
        "        subnetworks correlations but at the same time decrease capacity of every individual model.\n",
        "\n",
        "    Shape:\n",
        "        * Input: (N, C)\n",
        "        * Output: (N, C) (same shape as input)\n",
        "\n",
        "    Examples:\n",
        "\n",
        "    >>> m = Masksembles1D(16, 4, 2.0)\n",
        "    >>> input = torch.ones([4, 16])\n",
        "    >>> output = m(input)\n",
        "\n",
        "\n",
        "    References:\n",
        "\n",
        "    [1] `Masksembles for Uncertainty Estimation`,\n",
        "    Nikita Durasov, Timur Bagautdinov, Pierre Baque, Pascal Fua\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels: int, n: int, scale: float):\n",
        "        super().__init__()\n",
        "        self.channels, self.n, self.scale = channels, n, scale\n",
        "        masks_np = common.generation_wrapper(channels, n, scale)\n",
        "        masks = torch.as_tensor(masks_np, dtype=torch.float32)\n",
        "        self.register_buffer('masks', masks, persistent=False)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        masks = self.masks.to(dtype=inputs.dtype, device=inputs.device)\n",
        "\n",
        "        batch = inputs.shape[0]\n",
        "        chunks = torch.chunk(inputs.unsqueeze(1), self.n, dim=0)\n",
        "        x = torch.cat(chunks, dim=1).permute(1, 0, 2)             # [n, ?, C]\n",
        "        x = x * masks.unsqueeze(1)\n",
        "        x = torch.cat(torch.split(x, 1, dim=0), dim=1)\n",
        "        return x.squeeze(0)\n"
      ],
      "metadata": {
        "id": "ZxZQ-0Fjsa11"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jw12ljXCaMg",
        "outputId": "df05375d-7bd1-4d14-c0d5-0a43388381ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MasksemblesCNN(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (mask1): Masksembles2D()\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (mask2): Masksembles2D()\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (mask3): Masksembles1D()\n",
            "  (fc): Linear(in_features=1600, out_features=10, bias=True)\n",
            ")\n",
            "\n",
            "Total parameters: 34,826\n",
            "Trainable parameters: 34,826\n"
          ]
        }
      ],
      "source": [
        "class MasksemblesCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "        self.mask1 = Masksembles2D(32, n=4, scale=2.0)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.mask2 = Masksembles2D(64, n=4, scale=2.0)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.mask3 = Masksembles1D(64 * 5 * 5, n=4, scale=2.0)\n",
        "        self.fc = nn.Linear(64 * 5 * 5, num_classes)  # 5Ã—5 after two 2Ã—2 pools from 28Ã—28 input\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.elu(self.conv1(x))\n",
        "        x = self.mask1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = F.elu(self.conv2(x))\n",
        "        x = self.mask2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.mask3(x)\n",
        "        x = F.softmax(self.fc(x), dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Instantiate and summarize\n",
        "model = MasksemblesCNN(num_classes=10)\n",
        "print(model)\n",
        "\n",
        "# Print parameter count\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP9Km-bGigRC"
      },
      "source": [
        "Training of Masksembles is not different from training of regular model. So we just use standard Pytorch training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 5. Training Setup <a id=\"training-setup\"></a>\n",
        "\n",
        "We configure:\n",
        "- **Loss**: `CrossEntropyLoss` (expects integer class indices, not oneâ€‘hot)\n",
        "- **Optimizer**: `Adam`\n",
        "- **DataLoaders**: miniâ€‘batches for train and validation (90/10 split)\n"
      ],
      "metadata": {
        "id": "Xa-4-JNFqE0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model, loss, optimizer\n",
        "model = MasksemblesCNN(num_classes=10)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training loop\n",
        "epochs = 3\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-trVv38qQYA",
        "outputId": "1a5198b6-ecfe-44a1-d155-693af748150d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MasksemblesCNN(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (mask1): Masksembles2D()\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (mask2): Masksembles2D()\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (mask3): Masksembles1D()\n",
              "  (fc): Linear(in_features=1600, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 6. Training Loop <a id=\"training-loop\"></a>\n",
        "\n",
        "Standard PyTorch loop:\n",
        "1. `model.train()`, forward pass\n",
        "2. Compute loss, `backward()`, `step()`\n",
        "3. Track accuracy\n",
        "Then switch to `model.eval()` for validation with `torch.no_grad()`.\n"
      ],
      "metadata": {
        "id": "tKxCoTOZqACI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss, train_correct = 0, 0\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * x_batch.size(0)\n",
        "        train_correct += (outputs.argmax(1) == y_batch).sum().item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss, val_correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in test_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(x_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            val_loss += loss.item() * x_batch.size(0)\n",
        "            val_correct += (outputs.argmax(1) == y_batch).sum().item()\n",
        "\n",
        "    # Print epoch stats\n",
        "    print(\n",
        "        f\"Epoch {epoch+1}/{epochs} \"\n",
        "        f\"- Train loss: {train_loss/len(train_loader.dataset):.4f}, \"\n",
        "        f\"Train acc: {train_correct/len(train_loader.dataset):.4f}, \"\n",
        "        f\"Val loss: {val_loss/len(test_loader.dataset):.4f}, \"\n",
        "        f\"Val acc: {val_correct/len(test_loader.dataset):.4f}\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG18Ftg_qNqQ",
        "outputId": "c775536c-861c-42c8-8723-29faa4c2b0ca"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 - Train loss: 1.5990, Train acc: 0.8808, Val loss: 1.5153, Val acc: 0.9487\n",
            "Epoch 2/3 - Train loss: 1.5098, Train acc: 0.9545, Val loss: 1.4981, Val acc: 0.9648\n",
            "Epoch 3/3 - Train loss: 1.4985, Train acc: 0.9646, Val loss: 1.4927, Val acc: 0.9695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25b935df"
      },
      "source": [
        "\n",
        "## 7. Evaluation & Inference <a id=\"eval-infer\"></a>\n",
        "\n",
        "For evaluation, we disable gradients (`torch.no_grad()`) and call `model.eval()` to turn off trainingâ€‘time behavior.  \n",
        "We then compute `argmax` over class logits to get predicted labels.\n",
        "\n",
        "### **Important Notes on Inference <a id=\"important\"></a>:** Batch tiling and mask assignment\n",
        "\n",
        "Masksembles layers divide each batch into *N* segments â€” one for each submodel (mask).  \n",
        "That means:\n",
        "- The **first** 1/N of the batch goes through the first mask,  \n",
        "- The **second** 1/N through the second mask, and so on.\n",
        "\n",
        "So, to obtain predictions from **all submodels**, you need to **tile the same input image** *N* times along the batch dimension.  \n",
        "Each of these replicated samples will be processed by a different submodel.  \n",
        "After inference, you can then collect the *N* outputs to see how each submodel predicts the same input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh3pGxWujZ0Z"
      },
      "source": [
        "Now, we will check that all of Masksembles' submodels would predict similar predictions for training samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "T3GNT8Z5kItR"
      },
      "outputs": [],
      "source": [
        "img = train_dataset[0][0] # just random image from training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "o0K51rvqkMIs",
        "outputId": "dae2318b-5899-4402-dd06-f73fad962cb2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.imshow(img.permute(1, 2, 0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfIVxwixkQdY"
      },
      "source": [
        "To acquire predictions from different submodels one should transform input (with shape [1, H, W, C]) into batch (with shape [M, H, W, C]) that consists of M copies of original input (H - height of image, W - width of image, C - number of channels).\n",
        "\n",
        "As we can see Masksembles submodels produce similar predictions for training set samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thkg1KJJjY85",
        "outputId": "f40287cb-fd9b-4301-c175-ef3488e37ee8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREDICTION OF 1 MODEL: 3 CLASS\n",
            "PREDICTION OF 2 MODEL: 5 CLASS\n",
            "PREDICTION OF 3 MODEL: 5 CLASS\n",
            "PREDICTION OF 4 MODEL: 5 CLASS\n"
          ]
        }
      ],
      "source": [
        "# Suppose img has shape (28, 28, 1)\n",
        "inputs = torch.tile(img[None], (4, 1, 1, 1))       # (4, 1, 28, 28)\n",
        "\n",
        "# Send to same device as model\n",
        "device = next(model.parameters()).device\n",
        "inputs_t = inputs.to(device)\n",
        "\n",
        "# Inference\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predictions = model(inputs_t)  # (4, num_classes)\n",
        "    predicted_classes = predictions.argmax(dim=1)\n",
        "\n",
        "# Print predictions\n",
        "for i, cls in enumerate(predicted_classes):\n",
        "    print(f\"PREDICTION OF {i+1} MODEL: {cls.item()} CLASS\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH81gNyGlcnh"
      },
      "source": [
        "On out-of-distribution samples Masksembles should produce predictions with high variance, let's check it on complex samples from MNIST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "l9rbe1s9fBDf"
      },
      "outputs": [],
      "source": [
        "img = np.array(np.load(\"./complex_sample_mnist.npy\")[::-1, ::-1])\n",
        "img = torch.tensor(img).permute(2, 0, 1).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "n6YghcdrfdkZ",
        "outputId": "d0cda917-f87b-40c9-a424-c5a2c214f644"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7c907c505d00>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.imshow(img.permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65Y_-0h9U96h",
        "outputId": "559893c1-38d5-48fa-bde9-db0f26c421bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREDICTION OF 1 MODEL: 3 CLASS\n",
            "PREDICTION OF 2 MODEL: 5 CLASS\n",
            "PREDICTION OF 3 MODEL: 5 CLASS\n",
            "PREDICTION OF 4 MODEL: 5 CLASS\n"
          ]
        }
      ],
      "source": [
        "# Suppose img has shape (28, 28, 1)\n",
        "# Duplicate it 4 times\n",
        "inputs = torch.tile(img[None], (4, 1, 1, 1))   # (4, 28, 28, 1)\n",
        "\n",
        "# Move to same device as model\n",
        "device = next(model.parameters()).device\n",
        "inputs_t = inputs.to(device)\n",
        "\n",
        "# Run the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predictions = model(inputs_t)            # shape: (4, num_classes)\n",
        "    predicted_classes = predictions.argmax(dim=1)  # argmax over class dimension\n",
        "\n",
        "# Print results like in TensorFlow\n",
        "for i, cls in enumerate(predicted_classes):\n",
        "    print(f\"PREDICTION OF {i+1} MODEL: {cls.item()} CLASS\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e0fe845"
      },
      "source": [
        "\n",
        "## 8. Next Steps <a id=\"next-steps\"></a>\n",
        "\n",
        "- Try different `n` and `scale` to explore accuracy vs. diversity.  \n",
        "- Aggregate submodel logits (mean/median) for stronger predictions.  \n",
        "- Visualize perâ€‘submodel confidence for **uncertainty estimation**.  \n",
        "- Swap MNIST for CIFARâ€‘10 or your own dataset to test generalization.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}